{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads in the HTML files from the hard drive (not the web). This will need some work to read the docket entries, but that work can all be done within the scrape_page method; further web queries are probably unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_mac_page(filename):\n",
    "    \"\"\"\n",
    "    Open the MA Appellate Court html file and Soup it as beautifully as possible\n",
    "    \n",
    "    Input:\n",
    "        filename: The filename to parse\n",
    "    Output:\n",
    "        A dictionary of the items found in the case page\n",
    "        A list of dated docket entries\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(open(filename), 'html.parser')\n",
    "    info = {}\n",
    "    \n",
    "    # Get case tags\n",
    "    header = soup.find('td', class_=\"largefont\")\n",
    "    if len(list(soup.find_all(\"td\", align=\"center\"))) < 2:\n",
    "        return {}, []\n",
    "    center_cells = list(soup.find_all(\"td\", align=\"center\")[1].stripped_strings)\n",
    "    info[\"Court Type\"] = header.b.contents[0]\n",
    "    info[\"Panel\"] = header.b.contents[1].text\n",
    "    info[\"Case Name\"] = center_cells[0]\n",
    "    info[\"Case Id\"] = center_cells[-1]\n",
    "    #info[\"Case Id\"] = center_cells[2]\n",
    "    \n",
    "    # Get court tags\n",
    "    tables = soup.find_all(\"table\", class_=\"lightborder\")\n",
    "    attr_table = tables[0]\n",
    "    for row in attr_table.find_all(\"tr\", valign=\"top\"): \n",
    "        items = row.find_all(\"b\")\n",
    "        for item in items:\n",
    "            k = item.text\n",
    "            v = item.next.next.text.strip()\n",
    "            info[k] = v\n",
    "    \n",
    "    # Get parties\n",
    "    parties_table = soup.find(\"table\", class_=\"lightborder\", cellpadding=\"5\")\n",
    "    if parties_table is None:\n",
    "        return {}, []\n",
    "    p_k = set([])\n",
    "    for row in parties_table.find_all(\"tr\")[1:]:\n",
    "        k = row.b.nextSibling.next.strip().split('/')[0]\n",
    "        v = row.b.text.strip()\n",
    "\n",
    "        if k in info:\n",
    "            info[k].append(v)\n",
    "        else: \n",
    "            info[k] = [v]\n",
    "            p_k.add(k)\n",
    "        \n",
    "        ext = re.search(r'(\\d+) Extensions, (\\d+) Days', row.text)\n",
    "        if ext:\n",
    "            info['%s Extensions' %(k)] = ext.group(1)\n",
    "            info['%s Extension Days' %(k)] = ext.group(2)\n",
    "    for k in p_k:\n",
    "        info[k] = \", \".join(info[k])\n",
    "    \n",
    "    # Get docket entries\n",
    "    docket = []\n",
    "    for table in tables:\n",
    "        if table.find('tr').text.find('DOCKET ENTRIES') >= 0:\n",
    "            for row in table.find_all(\"tr\")[2:]:\n",
    "                items = row.find_all(\"td\")\n",
    "                date = items[0].text.strip()\n",
    "                entry = \"\"\n",
    "                if len(items) >= 3:\n",
    "                    entry = str(items[2].text.strip())\n",
    "                    entry = re.sub(r\"\\s+\", \" \", entry, flags=re.UNICODE)\n",
    "                docket.append([info['Case Id'], date, entry])\n",
    "    for entry in docket:\n",
    "        if re.search('affirm', entry[2], re.IGNORECASE):\n",
    "            info['Has Affirm'] = 'Yes'\n",
    "        if re.search('revers', entry[2], re.IGNORECASE):\n",
    "            info['Has Reverse'] = 'Yes'\n",
    "    \n",
    "    return info, docket\n",
    "\n",
    "base = \"http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=\"\n",
    "folder = r'MA Appellate Court'\n",
    "cases = []\n",
    "dockets = []\n",
    "keys = set([])\n",
    "\n",
    "# Read in all the downloaded pages and print / process them\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".html\"):\n",
    "        fullname = os.path.join(folder, file)\n",
    "        case, docket = scrape_mac_page(fullname)\n",
    "        case['URL'] = base + file\n",
    "        keys.update(case.keys())\n",
    "        if 'Case Status' in case:\n",
    "            if re.search('rescript', case['Case Status'], re.IGNORECASE):\n",
    "                if 'Case Type' in case and case['Case Type'] == 'Criminal':\n",
    "                    cases.append(case)\n",
    "                    dockets += docket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'Case Name' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-57e6a63140b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmove_to_front\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m'Case Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Case Type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Case Status'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Entry Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Argued Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Status Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Decision Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Has Affirm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Has Reverse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove_to_front\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove_to_front\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#j = dockets.index(move_to_front[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'Case Name' is not in list"
     ]
    }
   ],
   "source": [
    "#Run interactive session to see population of data scraped from the websites\n",
    "\n",
    "\n",
    "import unicodecsv as csv\n",
    "\n",
    "keys = list(keys)\n",
    "move_to_front = ['Case Id', 'Case Name', 'Case Type', 'Case Status', 'Entry Date', 'Argued Date', 'Status Date', 'Decision Date', 'Has Affirm', 'Has Reverse']\n",
    "for i in range(len(move_to_front)):\n",
    "    j = keys.index(move_to_front[i])\n",
    "    #j = dockets.index(move_to_front[i])\n",
    "    keys[i], keys[j] = keys[j], keys[i]\n",
    "    #dockets[i],dockets[j] = dockets[j],dockets[i]\n",
    "\n",
    "# Write out the case csv\n",
    "with open('cases.csv', 'wb') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(cases)\n",
    "\n",
    "# Write out the case csv\n",
    "with open('dockets.csv', 'wb') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['Case Id', 'Date', 'Entry'])\n",
    "    for entry in dockets:\n",
    "        writer.writerow(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversals by lower court judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot bar of lower court judge reversals\n",
    "num_cases = dict()\n",
    "num_reversals = dict()\n",
    "for case in cases:\n",
    "    if 'Lower Ct Judge' in case and len(case['Lower Ct Judge']) > 0:\n",
    "        jname = case['Lower Ct Judge']\n",
    "        if jname not in num_cases:\n",
    "            num_cases[jname] = 0\n",
    "        if jname not in num_reversals:\n",
    "            num_reversals[jname] = 0\n",
    "        num_cases[jname] += 1\n",
    "        if 'Has Reverse' in case:\n",
    "            num_reversals[jname] += 1\n",
    "\n",
    "# Calculate the reversal percentage\n",
    "perc_rev = dict()\n",
    "perc_rev_filter = dict()\n",
    "for jname in num_cases.keys():\n",
    "    if num_cases[jname] >= 0:\n",
    "        perc_rev[jname] = num_reversals[jname] / num_cases[jname]\n",
    "    if num_cases[jname] >= 5:\n",
    "        perc_rev_filter[jname] = num_reversals[jname] / num_cases[jname]\n",
    "\n",
    "# Get basic stats of each category\n",
    "a_t = list(num_cases.values())\n",
    "a_r = list(num_reversals.values())\n",
    "a_p = list(perc_rev.values())\n",
    "print(\"Total cases:\")\n",
    "print(\"Minimum: %s\" %(np.min(a_t)))\n",
    "print(\"Maximum: %s\" %(np.max(a_t)))\n",
    "print(\"Mean: %s\" %(np.mean(a_t)))\n",
    "print(\"Median: %s\" %(np.median(a_t)))\n",
    "print(\"Reversed cases:\")\n",
    "print(\"Minimum: %s\" %(np.min(a_r)))\n",
    "print(\"Maximum: %s\" %(np.max(a_r)))\n",
    "print(\"Mean: %s\" %(np.mean(a_r)))\n",
    "print(\"Median: %s\" %(np.median(a_r)))\n",
    "print(\"Reversal percentages:\")\n",
    "print(\"Minimum: %s\" %(np.min(a_p)))\n",
    "print(\"Maximum: %s\" %(np.max(a_p)))\n",
    "print(\"Mean: %s\" %(np.mean(a_p)))\n",
    "print(\"Median: %s\" %(np.median(a_p)))\n",
    "\n",
    "# Get cumulative distribution of each category\n",
    "n = len(a_t)\n",
    "a_t = sorted(a_t)\n",
    "c_t = [np.sum(a_t[:i]) / np.sum(a_t) for i in range(n)]\n",
    "a_r = sorted(a_r)\n",
    "c_r = [np.sum(a_r[:i]) / np.sum(a_t) for i in range(n)]\n",
    "#a_p = sorted(a_p)\n",
    "#c_p = [np.sum(a_p[:i]) / np.sum(a_p) for i in range(n)]\n",
    "\n",
    "# Graph the distribution\n",
    "plt.figure(dpi=500)\n",
    "p1 = plt.plot(range(n), a_t)\n",
    "p2 = plt.plot(range(n), a_r)\n",
    "#p3 = plt.plot(range(n), c_p)\n",
    "plt.legend((p1[0], p2[0]), ('Total Cases', 'Reversed Cases'))\n",
    "plt.title(\"Case Distribution by Judge\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the top 10 judges by percentage of reversals\n",
    "top_judge = sorted(perc_rev_filter.items(), key=lambda x: -x[1])\n",
    "top_judge = [li[0] for li in top_judge[:10]]\n",
    "\n",
    "# Graph them (stacked bar) over total cases\n",
    "plt.figure(figsize=(21, 4), dpi=500)\n",
    "p1 = plt.bar(top_judge, [num_cases[j] for j in top_judge], 0.5)\n",
    "p2 = plt.bar(top_judge, [num_reversals[j] for j in top_judge], 0.5)\n",
    "#plt.xticks(range(1, k+1), ['Cluster %s (%s)' %(i + 1, str(int(100 * num_rev[i] / len(groupings_dockets[i]))) + '%') for i in range(k)])\n",
    "plt.legend((p1[0], p2[0]), ('Total Cases', 'Reversed Cases'))\n",
    "plt.title(\"Reversals by Judge\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some labeling and graphing utilities I developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "ENGLISH_STOP_WORDS = frozenset([\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"])\n",
    "\n",
    "def extract_groupings(X, corpus, k, labels):\n",
    "    \"\"\"\n",
    "    Return data points and metadata grouped by cluster\n",
    "    \n",
    "    Plus some other logging that I found useful\n",
    "    \n",
    "    I'm also appending the sample representative to the list of clusters for ease of display\n",
    "    \n",
    "    Input:\n",
    "        X = data points\n",
    "        corpus = the reviews for each restaurant\n",
    "        k = number of clusters\n",
    "        labels = cluster number of each point in X\n",
    "    Output:\n",
    "        Two list of lists, where the ith sublist is all data points (or title) in the ith cluster\n",
    "    \"\"\"\n",
    "    # Grouping of points by cluster and metadata by cluster\n",
    "    groupings = [[] for i in range(k)]\n",
    "    corpora = [[] for i in range(k)]\n",
    "    winners = []\n",
    "    \n",
    "    # Intra-cluster similarity scores\n",
    "    iscore = dict()\n",
    "    \n",
    "    # Sort each point (and associated metadata) into bins for each cluster label\n",
    "    for i in range(len(X)):\n",
    "        label = labels[i]\n",
    "        \n",
    "        # Add this point to the cluster based on its label\n",
    "        groupings[label].append(X[i])\n",
    "        corpora[label].append(corpus[i])\n",
    "    \n",
    "    # Print (to file) the sample representative of each cluster\n",
    "    with open('reps.txt', 'w') as repfile:\n",
    "        for j in range(k):\n",
    "            # Score each case in the cluster by intra-cluster similarity\n",
    "            scores = np.sum(cosine_similarity(groupings[j], groupings[j]), axis=0)\n",
    "            \n",
    "            # The case with the maximum such score shall be considered the sample representative\n",
    "            index = np.argmax(scores)\n",
    "            repfile.write('Cluster %s Representative:\\n' %(j + 1))\n",
    "            repfile.write(corpora[j][index])\n",
    "            repfile.write('\\n')\n",
    "            repfile.write('\\n')\n",
    "            winners.append(groupings[j][index])\n",
    "            \n",
    "            # Save all the scores\n",
    "            for i in range(len(scores)):\n",
    "                iscore[corpora[j][i].split(' \\n')[0]] = scores[i]\n",
    "    \n",
    "    # Score the value of each term within the groupings and get the most meaningful terms for each cluster\n",
    "    titles = ['' for i in range(k)]\n",
    "    for i in range(k):\n",
    "        # Highest count words\n",
    "        cv = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, token_pattern=r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b').fit(corpora[i])\n",
    "        matrix = cv.transform(corpora[i])\n",
    "        smat = matrix.sum(axis=0)\n",
    "        wf = [(word, smat[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
    "        l = sorted(wf, key=lambda x: -x[1])\n",
    "        titles[i] = [li[0] for li in l[:8]]\n",
    "    \n",
    "    # Print (to file) the cluster and score of each case\n",
    "    with open('scores.csv', 'wb') as scorefile:\n",
    "        writer = csv.writer(scorefile)\n",
    "        writer.writerow(['Case ID', 'Cluster', 'ICS Score', 'Has Reverse', 'Has Affirm'])\n",
    "        for i in range(len(X)):\n",
    "            case_id = corpus[i].split(' \\n')[0]\n",
    "            ha = re.search('revers', entry[2], re.IGNORECASE)\n",
    "            writer.writerow([case_id, labels[i] + 1, iscore[case_id], 'Yes' if re.search('revers', corpus[i], re.IGNORECASE) else 'No', 'Yes' if re.search('affirm', corpus[i], re.IGNORECASE) else 'No'])\n",
    "    \n",
    "    return groupings + [winners], titles + ['Representatives']\n",
    "\n",
    "def plot_clustering(k, groupings, labels, title, axes=[2,1]):\n",
    "    \"\"\"\n",
    "    Plot the given k clusters on a 16x16 plot.\n",
    "    \n",
    "    Input:\n",
    "        k = the number of clusters\n",
    "        groupings = list of lists corresponding to the points in each cluster\n",
    "        labels = the title of each cluster\n",
    "        title = the title of the plot\n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # This size seems quite reasonable/readable\n",
    "    plt.figure(figsize=(16, 16), dpi=500)\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.xlabel('Component %s' %(axes[0] + 1))\n",
    "    plt.ylabel('Component %s' %(axes[1] + 1))\n",
    "    \n",
    "    # Store the plot results so we can label them later\n",
    "    legend = []\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i in range(k):\n",
    "        plot = None\n",
    "        if 'Representatives' == labels[i]:\n",
    "            plot = plt.scatter([entry[axes[0]] for entry in groupings[i]], [entry[axes[1]] for entry in groupings[i]], alpha=1, c='w', edgecolors='b')#, color=colors[i], marker=markers[i], s=8) \n",
    "        else:\n",
    "            plot = plt.scatter([entry[axes[0]] for entry in groupings[i]], [entry[axes[1]] for entry in groupings[i]], alpha=0.5)#, color=colors[i], marker=markers[i], s=8)\n",
    "        legend.append(plot)\n",
    "    \n",
    "    # Label each cluster\n",
    "    plt.legend(legend, labels, fancybox=True, framealpha=0.5)\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cluster our ma-appellatecourts.org data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import mixture\n",
    "from sklearn import metrics\n",
    "import scipy.cluster.hierarchy as hierarchy\n",
    "\n",
    "corpus_dockets = []\n",
    "last = ''\n",
    "for docket in dockets:\n",
    "    if last != docket[0]:\n",
    "        last = docket[0]\n",
    "        corpus_dockets.append('%s' %(docket[0]))\n",
    "    corpus_dockets[-1] += ' \\n' + docket[2]\n",
    "\n",
    "# Compute the tf-idf scores of the opinions\n",
    "# Using up to trigrams to account for adverbs and for legal terms\n",
    "fe_tfv = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, min_df=0.01, max_df = 0.5, ngram_range=(1, 3), token_pattern=r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b')\n",
    "tfidf_dockets = fe_tfv.fit_transform(corpus_dockets)\n",
    "\n",
    "# Compute the LSA of the scored opinions\n",
    "# After 4 components, we don't get much more ROI (plot leading to this conclusion is commented out below)\n",
    "# min/max document frequency play a huge role here\n",
    "n_c = 4\n",
    "dc_tsvd = TruncatedSVD(n_components=n_c)\n",
    "lsa_dockets = dc_tsvd.fit_transform(tfidf_dockets)\n",
    "\n",
    "# This is the code we would use to graph the singular values\n",
    "#fe_tfv = TfidfVectorizer(stop_words='english', min_df = 0.01, max_df = 0.5, token_pattern=r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b')\n",
    "#tfidf_dockets = fe_tfv.fit_transform(corpus_dockets)\n",
    "#dc_tsvd = TruncatedSVD(n_components=50)\n",
    "#lsa_dockets = dc_tsvd.fit_transform(tfidf_dockets)\n",
    "#plt.plot(range(1,51), dc_tsvd.singular_values_)\n",
    "#print(dc_tsvd.singular_values_)\n",
    "\n",
    "# Cluster via cosine/hiearchical\n",
    "# Best silhouette score is found with k=3, with 5 also being a local maximum\n",
    "# Plot leading to this conclusion commented out below\n",
    "k = 4\n",
    "linkage_dockets = hierarchy.linkage(lsa_dockets, \"average\", metric=\"cosine\")\n",
    "hier_dockets = hierarchy.fcluster(linkage_dockets, k, criterion='maxclust') - 1\n",
    "groupings_dockets, titles_dockets = extract_groupings(lsa_dockets, corpus_dockets, k, hier_dockets)\n",
    "#plot_clustering(k, groupings_dockets, titles_dockets, 'Cosine Similarity of Docket Entries')\n",
    "\n",
    "# This is the code we would use to display silhouette scores per k for cosine\n",
    "#x = list(range(2,15))\n",
    "#y = []\n",
    "#linkage_dockets = hierarchy.linkage(lsa_dockets, \"average\", metric=\"cosine\")\n",
    "#for i in x:\n",
    "#    hier = hierarchy.fcluster(linkage_dockets, i, criterion='maxclust') - 1\n",
    "#    y.append(metrics.silhouette_score(lsa_dockets, hier))\n",
    "#_ = plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = [0,2]\n",
    "\n",
    "plot_clustering(k + 1, groupings_dockets, titles_dockets, 'Cosine Similarity of Docket Entries', axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and transform reversed cases\n",
    "corpus_reversed = [docket for docket in corpus_dockets if re.search('revers', docket, re.IGNORECASE)]\n",
    "tfidf_reversed = fe_tfv.transform(corpus_reversed)\n",
    "lsa_reversed = dc_tsvd.transform(tfidf_reversed)\n",
    "\n",
    "plot_clustering(2, [lsa_dockets, lsa_reversed], ['All Cases', 'Reversed Cases'], 'Locality of Reversals', axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rev = [0] * k\n",
    "\n",
    "for i in range(len(corpus_dockets)):\n",
    "    if re.search('revers', corpus_dockets[i], re.IGNORECASE):\n",
    "        num_rev[hier_dockets[i]] += 1\n",
    "\n",
    "p1 = plt.bar(range(1, k+1), [len(groupings_dockets[i]) for i in range(k)], 0.25)\n",
    "p2 = plt.bar(range(1, k+1), num_rev, 0.25)\n",
    "plt.xticks(range(1, k+1), ['Cluster %s (%s)' %(i + 1, str(int(100 * num_rev[i] / len(groupings_dockets[i]))) + '%') for i in range(k)])\n",
    "plt.legend((p1[0], p2[0]), ('Total Cases', 'Reversed Cases'))\n",
    "plt.title(\"Reversals by Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for other features correlating with reversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import datetime\n",
    "\n",
    "# Convert case features into something measurable\n",
    "X_case = []\n",
    "for case in cases:\n",
    "    if 'Entry Date' not in case or 'Decision Date' not in case or len(case['Entry Date']) <= 0 or len(case['Decision Date']) <= 0:\n",
    "        continue\n",
    "    \n",
    "    # Features\n",
    "    a = []\n",
    "    # Duration\n",
    "    dt0 = datetime.strptime(case['Entry Date'], '%m/%d/%Y')\n",
    "    dt1 = datetime.strptime(case['Decision Date'], '%m/%d/%Y')\n",
    "    a.append(int((dt1-dt0).days))\n",
    "    # Prosecution extensions\n",
    "    a.append(int(case['Plaintiff Extensions']) if 'Plaintiff Extensions' in case else 0)\n",
    "    a.append(int(case['Plaintiff Extension Days']) if 'Plaintiff Extension Days' in case else 0)\n",
    "    # Defense extensions\n",
    "    a.append(int(case['Defendant Extensions']) if 'Defendant Extensions' in case else 0)\n",
    "    a.append(int(case['Defendant Extension Days']) if 'Defendant Extension Days' in case else 0)\n",
    "    # Classification\n",
    "    a.append(1 if 'Has Reverse' in case else 0)\n",
    "    \n",
    "    X_case.append(np.array(a))\n",
    "\n",
    "# Shuffle and split into feature/class\n",
    "X_case = np.array(X_case)\n",
    "np.random.shuffle(X_case)\n",
    "X_case, Y_case = np.split(X_case, [-1], axis=1)\n",
    "\n",
    "# Index of the beginning of the test holdout data\n",
    "test = int(len(X_case) * 0.8)\n",
    "\n",
    "# Train a decision tree on the data\n",
    "c_dt = tree.DecisionTreeClassifier()\n",
    "c_dt = c_dt.fit(X_case[:test], Y_case[:test])\n",
    "\n",
    "# ROC on test data seems to suggest this isn't really so useful just yet\n",
    "print(c_dt.score(X_case[:test], Y_case[:test]))\n",
    "print(c_dt.score(X_case[test:], Y_case[test:]))\n",
    "print(roc_auc_score(c_dt.predict(X_case[test:]), Y_case[test:], average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is for parsing SJC opinion pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_lexis_page(filename):\n",
    "    \"\"\"\n",
    "    Open the Lexis html file and Soup it as beautifully as possible\n",
    "    \n",
    "    Input:\n",
    "        filename: The filename to parse\n",
    "    Output:\n",
    "        A dictionary of the items found in the case page\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(open(filename, 'rb'), 'html.parser')\n",
    "    info = {}\n",
    "    \n",
    "    # Get document text\n",
    "    doctext = soup.find(\"div\", {\"class\": \"document-text\"})\n",
    "    # TODO: Figure out why some documents return None from the previous step\n",
    "    if not doctext:\n",
    "        return {}\n",
    "    \n",
    "    # Parse metadata\n",
    "    title = doctext.find(\"h1\", {\"id\": \"SS_DocumentTitle\"}).text.strip()\n",
    "    docinfo = doctext.find_all(\"p\", {\"class\": \"SS_DocumentInfo\"})\n",
    "    court = docinfo[0].text.strip()\n",
    "    dates = docinfo[1].text.strip().split(';')\n",
    "    case = docinfo[2].text.strip()\n",
    "    info['Case Title'] = title\n",
    "    info['Court'] = court\n",
    "    # TODO: Fix date parsing\n",
    "    #info['Date Argued'] = dates[0]\n",
    "    #info['Date Decided'] = dates[1]\n",
    "    info['Case Number'] = case\n",
    "    reporter = []\n",
    "    for sp in doctext.find_all(\"span\", {\"class\": \"SS_NonPaginatedRptr\"}):\n",
    "        reporter.append(sp.text.strip())\n",
    "    # TODO: Get more from this section\n",
    "    info['Reporter'] = ' | '.join(reporter)\n",
    "    # TODO: Find out how to parse Prior History and similar (e.g. subsequent history)\n",
    "    #prior = doctext.find_all(\"p\", {\"class\": \"SS_InlineText\"})[-1].text\n",
    "    #prior = re.sub(r\"\\s+\", \" \", prior, flags=re.UNICODE)\n",
    "    #info['Prior History'] = prior\n",
    "    \n",
    "    \n",
    "    #start = doctext.find(\"span\", id=\"JUMPTO_Counsel\")\n",
    "    #for i in range(25):\n",
    "    #    print(str(start.next_sibling).strip())\n",
    "    #    start = start.next_sibling\n",
    "    #here get stuff until br (end of category) and span (end of section)\n",
    "    \n",
    "    # TODO: Parse headnotes\n",
    "    \n",
    "    # Parse opinions\n",
    "    info['Opinion Author'] = get_text_after_span(doctext, \"JUMPTO_Opinionby\")\n",
    "    info['Opinion'] = get_text_after_id(doctext, \"JUMPTO_Opinion\")\n",
    "    info['Concuring Opinion Author'] = get_text_after_span(doctext, \"JUMPTO_Concurby\")\n",
    "    info['Concurring Opinion'] = get_text_after_id(doctext, \"JUMPTO_Concur\")\n",
    "    info['Dissenting Opinion Author'] = get_text_after_span(doctext, \"JUMPTO_Dissentby\")\n",
    "    info['Dissenting Opinion'] = get_text_after_id(doctext, \"JUMPTO_Dissent\")\n",
    "    \n",
    "    return info\n",
    "\n",
    "def get_text_after_span(document, s_id):\n",
    "    \"\"\"\n",
    "    Get the text immediately following some span with given id\n",
    "    \n",
    "    Input:\n",
    "        document: The section of text\n",
    "        s_id: The id of the span\n",
    "    Output:\n",
    "        The text immediately following said element (or the empty string if the id does not exist)\n",
    "    \"\"\"\n",
    "    start = document.find(\"span\", id=s_id)\n",
    "    if not start:\n",
    "        return \"\"\n",
    "    return str(start.next_sibling).strip()\n",
    "\n",
    "def get_text_after_id(document, e_id):\n",
    "    \"\"\"\n",
    "    Get the text in the paragraphs immediately following some element with given id\n",
    "    \n",
    "    Input:\n",
    "        document: The section of text\n",
    "        e_id: The id of the span\n",
    "    Output:\n",
    "        The text immediately following said element (or the empty string if the id does not exist)\n",
    "    \"\"\"\n",
    "    start = document.find(id=e_id)\n",
    "    if not start:\n",
    "        return \"\"\n",
    "    element = start.next_sibling\n",
    "    ps = []\n",
    "    while element and element.name == 'p':\n",
    "        text = element.text.strip()\n",
    "        text = re.sub(r\"\\s+\", \" \", text, flags=re.UNICODE)\n",
    "        ps.append(text)\n",
    "        element = element.next_sibling\n",
    "    return \" %%% \".join(ps)\n",
    "\n",
    "#base = \"http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=\"\n",
    "folder = 'Reversal Opinions HTML'\n",
    "cases = []\n",
    "keys = set([])\n",
    "\n",
    "# Read in all the downloaded pages and print / process them\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".html\"):\n",
    "        fullname = os.path.join(folder, file)\n",
    "        case = scrape_lexis_page(fullname)\n",
    "        keys.update(case.keys())\n",
    "        cases.append(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodecsv as csv\n",
    "\n",
    "print(keys)\n",
    "\n",
    "# Write out the csv\n",
    "with open('sjc-opinions.csv', 'wb') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
